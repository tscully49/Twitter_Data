{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Data Research\n",
    "\n",
    "---\n",
    "\n",
    "### Question: \n",
    "Each job_id in the twitter data has a 'description' tied to it.  Can we predict what the 'description' of the tweet is based on the text?\n",
    "\n",
    "##### First, I load some python libraries, and choose which job_ids I want to analyze.  I then create a connection to the database.*\n",
    "\n",
    "***note** I ended up connecting to the subset database because the main database kept timing out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymysql as mdb\n",
    "\n",
    "allTweets = {}\n",
    "\n",
    "topics = {\n",
    "    1160: \"#tornado\",\n",
    "    2577: \"#worldseries\",\n",
    "    2616: \"autism\",\n",
    "    4244: \"alabama\",\n",
    "    4245: \"#istandwithloftin\",\n",
    "    4246: \"@istandwithloftin\",\n",
    "    4259: \"PorteOuverte\", \n",
    "    4253: \"mizzouunited\", \n",
    "    4255: \"concernedstudent1950\", \n",
    "    4258: \"prayformizzou\",\n",
    "    4257: \"#MIZZOU\"\n",
    "}\n",
    "\n",
    "allDescriptions = {\n",
    "    1160: \"weather\",\n",
    "    2577: \"baseball\",\n",
    "    2616: \"autism\",\n",
    "    4244: \"Florida Hurricane\",\n",
    "    4245: \"Loftin\",\n",
    "    4246: \"Loftin\",\n",
    "    4259: \"Paris 2015\", \n",
    "    4253: \"Mizzou\", \n",
    "    4255: \"Mizzou\", \n",
    "    4258: \"Mizzou\",\n",
    "    4257: \"Mizzou\"\n",
    "}\n",
    "\n",
    "connection = mdb.connect('128.206.116.195', 'tg4_ro', '?3stEt7!3hUbRa-R', 'tw4_db')\n",
    "subsetConnection = mdb.connect('opendata.missouri.edu', 'datascience', 'datascience', 'datascience')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next, I went through and collected the job_id and tweet text from all of the tweets in each job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#worldseries\n",
      "prayformizzou\n",
      "PorteOuverte\n",
      "alabama\n",
      "#istandwithloftin\n",
      "@istandwithloftin\n",
      "autism\n",
      "#tornado\n",
      "#MIZZOU\n",
      "mizzouunited\n",
      "concernedstudent1950\n"
     ]
    }
   ],
   "source": [
    "for jobId, topic in topics.items():\n",
    "    sql = 'SELECT job_id, text FROM tweet WHERE job_id = ' + str(jobId)\n",
    "    allTweets[jobId] = pd.read_sql(sql, subsetConnection)\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2577 2527317 baseball\n",
      "4258 201833 Mizzou\n",
      "4259 354430 Paris 2015\n",
      "4244 7273542 Florida Hurricane\n",
      "4245 160 Loftin\n",
      "4246 0 Loftin\n",
      "4257 755779 Mizzou\n",
      "2616 7495015 autism\n",
      "1160 168882 weather\n",
      "4253 2269 Mizzou\n",
      "4255 248646 Mizzou\n"
     ]
    }
   ],
   "source": [
    "for key in allTweets:\n",
    "    print(str(key) + \" \" + str(len(allTweets[key])) + \" \" + allDescriptions[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, I need to create my training array and my test array.  The training array will hold the tweets and descriptions that I am going to use to create my model.  This should allow me to run the test array with my model and produce a high accuracy of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2527317\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "Done with: 2527317\n",
      "201833\n",
      "100000\n",
      "200000\n",
      "Done with: 201833\n",
      "354430\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "Done with: 354430\n",
      "7273542\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n",
      "4300000\n",
      "4400000\n",
      "4500000\n",
      "4600000\n",
      "4700000\n",
      "4800000\n",
      "4900000\n",
      "5000000\n",
      "5100000\n",
      "5200000\n",
      "5300000\n",
      "5400000\n",
      "5500000\n",
      "5600000\n",
      "5700000\n",
      "5800000\n",
      "5900000\n",
      "6000000\n",
      "6100000\n",
      "6200000\n",
      "6300000\n",
      "6400000\n",
      "6500000\n",
      "6600000\n",
      "6700000\n",
      "6800000\n",
      "6900000\n",
      "7000000\n",
      "7100000\n",
      "7200000\n",
      "Done with: 7273542\n",
      "160\n",
      "Done with: 160\n",
      "755779\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "Done with: 755779\n",
      "7495015\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n",
      "4300000\n",
      "4400000\n",
      "4500000\n",
      "4600000\n",
      "4700000\n",
      "4800000\n",
      "4900000\n",
      "5000000\n",
      "5100000\n",
      "5200000\n",
      "5300000\n",
      "5400000\n",
      "5500000\n",
      "5600000\n",
      "5700000\n",
      "5800000\n",
      "5900000\n",
      "6000000\n",
      "6100000\n",
      "6200000\n",
      "6300000\n",
      "6400000\n",
      "6500000\n",
      "6600000\n",
      "6700000\n",
      "6800000\n",
      "6900000\n",
      "7000000\n",
      "7100000\n",
      "7200000\n",
      "7300000\n",
      "7400000\n",
      "Done with: 7495015\n",
      "168882\n",
      "100000\n",
      "Done with: 168882\n",
      "2269\n",
      "Done with: 2269\n",
      "248646\n",
      "100000\n",
      "200000\n",
      "Done with: 248646\n"
     ]
    }
   ],
   "source": [
    "training = []\n",
    "test = []\n",
    "\n",
    "for key in allTweets:\n",
    "    total = len(allTweets[key])\n",
    "    count = 0\n",
    "    \n",
    "    description = allDescriptions[key]\n",
    "    \n",
    "    breakingPoint = total*(3/4)\n",
    "    \n",
    "    if (total != 0):\n",
    "        print(total)\n",
    "        for key2, value2 in allTweets[key].iterrows():\n",
    "            temp = {\n",
    "                    'job_id' : allTweets[key].loc[key2, 'job_id'],\n",
    "                    'text' : allTweets[key].loc[key2, 'text'],\n",
    "                    'description' : description\n",
    "            }\n",
    "            \n",
    "            if (count < breakingPoint):\n",
    "                training.append(temp)\n",
    "            else:\n",
    "                test.append(temp)\n",
    "            count = count + 1\n",
    "            if (count % 100000 == 0):\n",
    "                print(count)\n",
    "        print(\"Done with: \" + str(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Documentation for the python Naive Bayes Library can be found [here](https://github.com/muatik/naive-bayes-classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from naiveBayesClassifier import tokenizer\n",
    "from naiveBayesClassifier.trainer import Trainer\n",
    "from naiveBayesClassifier.classifier import Classifier\n",
    "\n",
    "trainer = Trainer(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With the training and test sets complete, I can now import the Python naive Bayes library and train my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for item in training:\n",
    "    trainer.train(item['text'], item['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = Classifier(trainer.data, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With my model trained, I can now use my test set to see how accurate my model is with predicting the description based on the tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correctPredicted = 0\n",
    "\n",
    "for item in test:\n",
    "    res = classifier.classify(item['text'])\n",
    "    if res[0][0] == item['description']:\n",
    "        correctPredicted += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4504352\n",
      "4756964\n",
      "\n",
      "94.69% of all of the test tweets were predicted correctly\n"
     ]
    }
   ],
   "source": [
    "print(correctPredicted)\n",
    "print(len(test))\n",
    "print('\\n' + str(round(correctPredicted/len(test)*100,2)) + \"% of all of the test tweets were predicted correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The data set ended up being 94.69% correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
